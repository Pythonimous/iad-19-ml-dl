{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 2.0 Nikolaev Manual.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gqLv4tIVaXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb8nbSu4cp5q",
        "colab_type": "text"
      },
      "source": [
        "## y = x^2 approximation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjxxxif4b2a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.linspace(0, 1, 1000).reshape(1,-1)\n",
        "y_train = np.square(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya53OXMkcN0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOflrdOtceo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm38H5OPdVrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
        "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_RrygsfDlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Linear part of forward propagation.\n",
        "    Arguments:\n",
        "    A -- activations from previous layer / input data. dim: (previous layer, num of examples)\n",
        "    W -- weights matrix. dim: (current layer, previous layer)\n",
        "    b -- bias vector. dim: (current layer, 1)\n",
        "    Returns:\n",
        "    Z -- input of activation function;\n",
        "    cache -- stored for backward pass later\n",
        "    \"\"\"\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCPs8Z9Cf4cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQOG44TyiJ7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsMx6jc1hHfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation_forward(A_prev, W, b, activation):\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    if activation == 'relu':\n",
        "        A, activation_cache = relu(Z)\n",
        "    elif activation == 'sigmoid':\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQe7-6KghbOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Forward propagation.\n",
        "    X -- input data (input dim, number of examples);\n",
        "    parameters -- output of initialize_parameters()\n",
        "\n",
        "    Returns:\n",
        "    A_last: last post activation value;\n",
        "    caches -- list of caches for each step\n",
        "    \"\"\"\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], 'relu')\n",
        "        caches.append(cache)\n",
        "    \n",
        "    A_last, cache = activation_forward(A, parameters[f'W{L}'], parameters[f'b{L}'], 'sigmoid')\n",
        "    caches.append(cache)\n",
        "\n",
        "    return A_last, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfe-xjUijLA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mae(A_last, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    mae = abs(np.sum(A_last - Y) / m)\n",
        "    return mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMumzGcMlhAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_approx_error(A_last, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    approx = abs(1 - np.linalg.norm(A_last) / np.linalg.norm(Y)) # to avoid division by 0\n",
        "    return approx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNgzC55Gmakd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compute_mae(np.linspace(0.5, 1, 5).reshape(1,-1), np.linspace(0, 1, 5).reshape(1,-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_3KxcbwmjqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Linear portion of backprop for 1 layer.\n",
        "    dZ - gradient of the cost with respect to linear output (of current layer l)\n",
        "    cache - tuple of (A_prev, W, b) from forward of this layer\n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uiANui5qdLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "\n",
        "    dZ[Z<=0] = 0\n",
        "\n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGZfi3UsqxRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    Z = cache\n",
        "\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA*s*(1-s)\n",
        "\n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B6DzuqHp_B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Backprop for linear -> activation.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) for computing backward propagation\n",
        "    activation -- activation to be used in this layer: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == 'relu':\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "    else:\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dA, linear_cache)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Csy9r0rVsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_backward(A_last, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "\n",
        "    dA_last = ((A_last > Y).astype(np.int) * 2) - 1  # derivative for MAE: 1 if Ypred > Ytrue, -1 otherwise\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[f'dA{L-1}'], grads[f'dW{L}'], grads[f'db{L}']  = activation_backward(dA_last, current_cache, sigmoid)\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[f'dA{l+1}'], current_cache, 'relu')\n",
        "        grads[f'dA{l}'] = dA_prev_temp\n",
        "        grads[f'dW{l+1}'] = dW_temp\n",
        "        grads[f'db{l+1}'] = db_temp\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIbxi_mxt7Jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(L):\n",
        "        parameters[f'W{l+1}'] -= learning_rate * grads[f'dW{l+1}']\n",
        "        parameters[f'b{l+1}'] -= learning_rate * grads[f'db{l+1}']\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEYWklx1ufPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, Y, nodes_hidden, num_layers, learning_rate = 0.001, num_epochs = 1000, print_cost = False):\n",
        "    np.random.seed(42)\n",
        "    grads = {}\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "\n",
        "    layer_dims = [1] + [nodes_hidden]*num_layers + [1]\n",
        "\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    for i in range(1, num_epochs+1):\n",
        "        A_last, caches = model_forward(X, parameters)\n",
        "        cost = compute_mae(A_last, Y)\n",
        "\n",
        "        grads = model_backward(A_last, Y, caches)\n",
        "\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if print_cost and i % 200 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    print(f'Minimal cost: {min(costs)}, at {costs.index(min(costs)) + 1} epochs')\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO0wwFs8v-st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = model(x_train, y_train, 4, 10, num_epochs = 3000, print_cost = True)  # 2612 эпох - минимальное значение."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkXcgfrLY73x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}