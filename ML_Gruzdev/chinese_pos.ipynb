{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "poTzBVcTCpJU",
    "outputId": "1d3896c4-fcf6-4df9-9960-be6b49c6782a"
   },
   "outputs": [],
   "source": [
    "!pip3 install conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLYzNJ1LCQy-"
   },
   "source": [
    "## Фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dk-69H-oCQzD"
   },
   "source": [
    "В первую очередь используем иероглифы, которые, как известно, имеют в себе семантический компонент, и, таким образом, являются значимыми признаками. Список иероглифов извлечём с http://www.hanzicraft.com/lists/frequency - там 8943 наиболее распространённых иероглифов. (в Python своего встроенного списка нет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUWStIoCCQzG"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "sess = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Bgr0sgUCQzP"
   },
   "source": [
    "Возьмём четыре тысячи наиболее частых иероглифов (на сайте они выстроены по частоте), потому что все сразу брать смысла нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpWXZQm4CQzR"
   },
   "outputs": [],
   "source": [
    "page = sess.get('http://www.hanzicraft.com/lists/frequency')\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "hanzi = [ch.text.strip().split('\\n')[0] for ch in soup.findAll('li', class_='list')][:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "K282_wTJCQzY",
    "outputId": "fbbb7a7e-d480-4ef0-b4dc-e09a68215e94"
   },
   "outputs": [],
   "source": [
    "len(hanzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sdSIrv8CQzg"
   },
   "outputs": [],
   "source": [
    "hanzi_set = set(hanzi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQ0az1QnCQzl"
   },
   "source": [
    "Загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYTkF6JzCQzm"
   },
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nywXDovlFTtQ",
    "outputId": "4ab04c7f-f8e4-4819-f0b6-b776734e0b59"
   },
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-train.conllu'\n",
    "urllib.request.urlretrieve(train_url, filename = 'zh_gsd-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJaW3ayoCQzu"
   },
   "outputs": [],
   "source": [
    "with open(\"zh_gsd-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cntr = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lny_pGa2CQz2"
   },
   "outputs": [],
   "source": [
    "chinese_train = parse(cntr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8r7f8gGFCQz8",
    "outputId": "e2814ea4-73a6-4d9b-82ca-ce701372d167"
   },
   "outputs": [],
   "source": [
    "chinese_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcpqRfCGCQ0C"
   },
   "source": [
    "Для обучения будем использовать bag of characters, поскольку для иероглифов это просто предельно важно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9ljr1N1CQ0I"
   },
   "outputs": [],
   "source": [
    "chinese_train_words = [[w['form'] for w in i] for i in chinese_train]\n",
    "chinese_train_postags = [[w['upostag'] for w in i] for i in chinese_train]\n",
    "train_data = [(chinese_train_words[i], chinese_train_postags[i]) for i in range(len(chinese_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "hLZU3-heCQ0P",
    "outputId": "b081050a-858c-492f-b5c1-97109f889adf"
   },
   "outputs": [],
   "source": [
    "chinese_train_words[3], chinese_train_postags[3], train_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JcrUhqtCQ0V"
   },
   "source": [
    "Для иероглифов используем bag of characters. В основном иероглифы в одном слове не будут встречаться дважды, и словарь иероглифов довольно ограничен, так что этот подход довольно эффективен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dyYhpIcCQ0Y"
   },
   "outputs": [],
   "source": [
    "def features_list(word):\n",
    "    features = [0] * 4000\n",
    "    for ch in word:\n",
    "        if isinstance(ch, int) and len(features) == 4000: # в таком случае это не иероглиф, а число. достаточно единожды.\n",
    "            features.append(1)     # для чисел отдельная категория и своя часть речи.\n",
    "        elif (not isinstance(ch, int)) and len(features) == 4000:\n",
    "            features.append(0)\n",
    "            \n",
    "        if ch in hanzi_set: # быстрее membership checking. если иероглиф есть в списке:\n",
    "            ind = hanzi.index(ch) # соответсвтующая позиция - 1\n",
    "            features[ind] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_0Zz4IqCQ0c"
   },
   "source": [
    "Итого получаем на выходе вектор размерностью 4001: 4000 иероглифов + число/не число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t11dKCp2CQ0e"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq): # слова как вектор с тем, какие иероглифы в нём содержатся\n",
    "    idxs = [features_list(w) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28ZAVsUZCQ0i"
   },
   "outputs": [],
   "source": [
    "def prepare_tags(seq, tags_ix): # тэги по маппингу\n",
    "    idxs = [tags_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIHPSF2WCQ0n"
   },
   "source": [
    "Построим таблицу POS тэгов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rw5dBxwnCQ0p"
   },
   "outputs": [],
   "source": [
    "train_pos = [dic['upostag'] for dic in sum(chinese_train, [])] # какие части речи есть в тренировочном корпусе? их и метим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiIMT9rPCQ0t"
   },
   "outputs": [],
   "source": [
    "textlabels = list(set(train_pos))\n",
    "tag_to_ix = {}\n",
    "for i in range(len(textlabels)):\n",
    "    tag_to_ix[textlabels[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "JdjdyPSOCQ0w",
    "outputId": "eab94b7a-ef74-4554-a8b0-5771766f98cb"
   },
   "outputs": [],
   "source": [
    "tag_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GA2ghKSJCQ00"
   },
   "source": [
    "Получим тестовые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DVPzn9bEGBEC",
    "outputId": "66fcd9e6-5e1c-4e0a-87f3-1618aa8a7d38"
   },
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-test.conllu'\n",
    "urllib.request.urlretrieve(test_url, filename = 'zh_gsd-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4VlTOQwCQ02"
   },
   "outputs": [],
   "source": [
    "with open(\"zh_gsd-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f: #https://github.com/UniversalDependencies/UD_Chinese-GSD\n",
    "    cntst = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9P91D6YCQ05"
   },
   "outputs": [],
   "source": [
    "chinese_test = parse(cntst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A74QHzurCQ0-"
   },
   "outputs": [],
   "source": [
    "chinese_test_words = [[w['form'] for w in i] for i in chinese_test]\n",
    "chinese_test_postags = [[w['upostag'] for w in i] for i in chinese_test]\n",
    "test_data = [(chinese_test_words[i], chinese_test_postags[i]) for i in range(len(chinese_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6p8T_L7fCQ1C",
    "outputId": "37fda324-00d4-4b73-c9ca-294181fc6166",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chinese_test_words[2], chinese_test_postags[2], test_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfX1upg5CQ1H"
   },
   "source": [
    "Приступим к питорчу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78_44sqlCQ1J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGlrnSxpCQ1N"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 4001\n",
    "HIDDEN_DIM = 64\n",
    "VOCAB_SIZE = 4001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vL5tR20CQ1R"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_out, _ = self.lstm(sentence.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KG2y5CViCQ1V"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2G-rQyPbCQ1a",
    "outputId": "03c34ffe-9647-43ca-bd13-1b46a3fff01c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # scores before training\n",
    "    inputs = prepare_sequence(train_data[0][0])\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OGSLCxQCQ1e"
   },
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "uFaA5oPpCQ1i",
    "outputId": "9d255cb8-cdba-454b-d429-47c8f93da2d0"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in tqdm(train_data):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence)\n",
    "        targets = prepare_tags(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch {} has passed'.format(str(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "dJzXGnL_CQ1s",
    "outputId": "43dca265-f076-4dd7-946c-d28f6120efbd"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(train_data[0][0])\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    print(tag_scores[0]) # максимальное значение в тензоре соответствует предсказанной POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oHrQHunLCQ1w",
    "outputId": "7067ae21-94ab-481f-c71d-d696198498b6"
   },
   "outputs": [],
   "source": [
    "tags_predicted = []\n",
    "tags_true = []\n",
    "for d in tqdm(test_data):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tags = prepare_tags(d[1], tag_to_ix)\n",
    "        tags_true.append(tags.tolist())\n",
    "        \n",
    "        inputs = prepare_sequence(d[0])\n",
    "        tag_scores = model(inputs).tolist()\n",
    "        tags_encoded = [i.index(max(i)) for i in tag_scores] # максимальное значение в тензоре соответствует предсказанной POS\n",
    "        tags_predicted.append(tags_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CM8PfsTnCQ10",
    "outputId": "8ca71264-e48b-4a50-a3fa-68a03fb47a47"
   },
   "outputs": [],
   "source": [
    "tags_predicted[0], tags_true[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScQ3VSiBCQ1-"
   },
   "source": [
    "Для глобальной статистики мы можем и unnestн'уть списки. Это поможет определить, какие части речи определились правильно, а какие - нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdnJdnsCCQ2A"
   },
   "outputs": [],
   "source": [
    "tags_pred = sum(tags_predicted, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxoIwzpqCQ2D"
   },
   "outputs": [],
   "source": [
    "tags_actual = sum(tags_true, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vNjB4qzPCQ2I"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PgROjxCMCQ2M",
    "outputId": "ec70028f-1d37-428e-9880-e7f79cfa936c"
   },
   "outputs": [],
   "source": [
    "f1_score(tags_actual, tags_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ExLHtXTqCQ2Q",
    "outputId": "df750366-f3c4-4dfc-caaa-2adada3a67c3"
   },
   "outputs": [],
   "source": [
    "accuracy_score(tags_actual, tags_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt5xtO1HCQ2u"
   },
   "source": [
    "'PART': 0,\n",
    " 'SYM': 1,\n",
    " 'ADP': 2,\n",
    " 'ADJ': 3,\n",
    " 'NOUN': 4,\n",
    " 'CCONJ': 5,\n",
    " 'DET': 6,\n",
    " 'VERB': 7,\n",
    " 'PRON': 8,\n",
    " 'X': 9,\n",
    " 'NUM': 10,\n",
    " 'PUNCT': 11,\n",
    " 'ADV': 12,\n",
    " 'AUX': 13,\n",
    " 'PROPN': 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "icktmASmCQ2v",
    "outputId": "6c2bd839-fb2b-4e98-d09d-bc7370ab9edc"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(tags_actual, tags_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExUfcKRWCQ26"
   },
   "source": [
    "Для пяти эпох результаты очень хорошие. Это говорит об эффективности модели Bag of Characters для сильно основанного на иероглифах китайского языка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2AN46duCQ3A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1 Chinese POS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
